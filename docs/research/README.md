# AI Safety Research

This directory contains research materials that guide the long-term vision of circuitry-forge.

## Overview

Our north star is the comprehensive academic survey "A Researcher's Guide to the Frontiers of AI Safety and Interpretability" which covers:

1. **Foundational Concepts** - Core terminology: safety, alignment, robustness, interpretability
2. **Research Landscape** - Taxonomy of safety threats and interpretability methods
3. **Key Players** - Industry labs (Anthropic, OpenAI, Google) and academic institutions
4. **Researcher's Toolkit** - Essential resources, libraries, and benchmarks
5. **Open Problems** - Unsolved challenges in scalable oversight, robustness, and deception
6. **Case Study** - Wysa mental health AI as a real-world safety implementation

## How This Guides scenario-forge

While our current tool focuses on practical scenario generation, this research ensures we're building toward the most critical safety challenges:

- **Alignment**: Our scenarios test if AI systems pursue intended objectives
- **Robustness**: Our scenarios probe edge cases and distribution shifts
- **Interpretability**: Future integration with mechanistic interpretability tools
- **Safety Evaluation**: Our scenarios contribute to the broader safety ecosystem

## Key Takeaways for Implementation

1. **Data-Centric Alignment**: Quality of safety evaluation depends on quality of scenarios
2. **Constitutional AI**: Future versions may incorporate explicit safety principles
3. **Scalable Oversight**: Our scenarios help humans supervise increasingly capable AI
4. **Real-World Impact**: Focus on high-stakes domains like mental health

The full research document is available upon request but was omitted here for maintainability.